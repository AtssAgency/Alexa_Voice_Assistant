# ============================================================
# Alexa (Local Voice Assistant) — Unified Configuration
# ------------------------------------------------------------
# One file for all services: MAIN, LOADER, LOGGER, RMS, KWD, STT, LLM, TTS
# All services bind to loopback (127.0.0.1). Ports must match specs.
# ============================================================

# -----------------------------
# MAIN (entrypoint: main.py)
# -----------------------------
[main]
# forwarded to logger at bootstrap (info|debug)
log_level       = debug
logger_cmd      = python3 -m services.logger_service
loader_cmd      = python3 -m services.loader_service
# deadline for initial /log acceptance
wait_logger_ms  = 3000

# -----------------------------
# LOGGER SERVICE
# -----------------------------
[logger]
port                = 5000
# info: storyline only | debug: echo all
log_level           = debug
# dialog_*.log
dialog_prefix       = dialog_
# timestamps in app.log & memory.log
append_timestamps   = true

# -----------------------------
# LOADER (process manager)
# -----------------------------
[loader]
startup_order       = logger,kwd,stt,ollama,llm,tts
health_timeout_s    = 30
health_interval_ms  = 200
# with ±20% jitter
restart_backoff_ms  = 250,500,1000,2000
# failures within 60s => FAIL
crash_loop_threshold= 3
# kill stale PID on port bind conflicts
port_hygiene        = true
vram_probe_cmd      = nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits
warmup_llm          = true
# call KWD /on-system-ready after all OK
greeting_on_ready   = true

[paths]
python              = python3

# Per-service launch definitions (used by loader)
[svc.logger]
name    = LOGGER
cmd     = python -m services.logger_service
port    = 5000


[svc.kwd]
name    = KWD
cmd     = python -m services.kwd_service
port    = 5002
health  = http://127.0.0.1:5002/health

[svc.stt]
name    = STT
cmd     = python -m services.stt_service
port    = 5003
health  = http://127.0.0.1:5003/health

[svc.ollama]
name    = OLLAMA
cmd     = ollama serve
port    = 11434
# No /health endpoint that returns JSON like others
# loader will just check if process is alive
# (llm_service still does real availability check via /api/tags)

[svc.llm]
name    = LLM
cmd     = python -m services.llm_service
port    = 5004
health  = http://127.0.0.1:5004/health
warmup  = http://127.0.0.1:5004/warmup

[svc.tts]
name    = TTS
cmd     = python -m services.tts_service
port    = 5005
health  = http://127.0.0.1:5005/health


# -----------------------------
# KWD — Wake Word (openWakeWord)
# -----------------------------
[kwd]
bind_host           = 127.0.0.1
port                = 5002
model_path          = models/alexa_v0.1.onnx
# pre-adaptation
base_rms_threshold  = 0.035
cooldown_ms         = 1000
yes_phrases         = Yes?,Yes Master?,What's up?,I'm listening,Yo,Here!
# spoken once after all services ready
greeting_text       = Hi Master. Ready to serve.

[kwd.adaptive]
quiet_dbfs          = -60
noisy_dbfs          = -35
quiet_factor        = 0.80
noisy_factor        = 1.25

[kwd.deps]
logger_url          = http://127.0.0.1:5000
tts_url             = http://127.0.0.1:5005/speak
stt_url             = http://127.0.0.1:5003/start

# -----------------------------
# STT — Dialog Session Manager (faster-whisper)
# -----------------------------
[stt]
bind_host           = 127.0.0.1
port                = 5003
model_size          = small.en
# cuda | cpu
device              = cuda
# CPU-compatible compute type
compute_type        = float32
beam_size           = 1
vad_finalize_sec    = 2.0
chunk_length_s      = 15.0
follow_up_timer_sec = 4.0
word_timestamps     = false

[stt.audio]
device_index      = 4         
sample_rate       = 48000
frame_ms          = 32
dtype             = int16
dc_block          = true
min_frame_rms     = 0.003

[stt.adaptive]
quiet_dbfs          = -60
noisy_dbfs          = -35

[stt.vad]
silence_margin_db   = 8.0
energy_delta_db     = 6.0
engine            = silero      
start_threshold   = 0.6         
end_threshold     = 0.45        
min_start_frames  = 5           
max_silence_ms    = 800         
pre_roll_ms       = 400
speech_pad_ms     = 300


[stt.deps]
logger_url          = http://127.0.0.1:5000
llm_url             = http://127.0.0.1:5004/complete
tts_ws_speak        = ws://127.0.0.1:5005/speak-stream
tts_ws_events       = ws://127.0.0.1:5005/playback-events
kwd_url             = http://127.0.0.1:5002/start

# -----------------------------
# LLM — Streaming Completions (Ollama)
# -----------------------------
[llm]
bind_host           = 127.0.0.1
port                = 5004
ollama_base_url     = http://127.0.0.1:11434
request_timeout_s   = 120
warmup_enabled      = true
warmup_text         = ok

# Default sampling (can be overridden per request)
temperature         = 0.6
top_p               = 0.9
top_k               = 40
repeat_penalty      = 1.05
max_tokens          = 512

[llm.deps]
logger_url          = http://127.0.0.1:5000

# -----------------------------
# TTS — Streaming Speech (Kokoro)
# -----------------------------
[tts]
bind_host           = 127.0.0.1
port                = 5005
# default voice (configurable)
voice               = af_heart
sample_rate         = 24000
device              = cuda
# minimize VRAM
dtype               = float16
max_queue_text      = 64
max_queue_audio     = 8
chunk_chars         = 120
silence_pad_ms      = 60
# primary mode is STT push
allow_llm_pull      = false

# ONNX assets
model_path          = models/kokoro-v1.0.fp16.onnx
voices_path         = models/voices-v1.0.bin
quant_preference    = fp16

[tts.deps]
logger_url          = http://127.0.0.1:5000
stt_ws_events_allow = true
