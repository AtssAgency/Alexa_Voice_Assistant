# ============================================================
# Alexa (Local Voice Assistant) — Unified Configuration
# ------------------------------------------------------------
# One file for all services: MAIN, LOADER, LOGGER, RMS, KWD, STT, LLM, TTS
# All services bind to loopback (127.0.0.1). Ports must match specs.
# ============================================================

# -----------------------------
# MAIN (entrypoint: main.py)
# -----------------------------
[main]
# forwarded to logger at bootstrap (info|debug)
log_level       = debug
logger_cmd      = python3 -m services.logger_service
loader_cmd      = python3 -m services.loader_service
# deadline for initial /log acceptance
wait_logger_ms  = 3000

# -----------------------------
# LOGGER SERVICE
# -----------------------------
[logger]
port                = 5000
# info: storyline only | debug: echo all
log_level           = debug
# dialog_*.log
dialog_prefix       = dialog_
# timestamps in app.log & memory.log
append_timestamps   = true

# -----------------------------
# LOADER (process manager)
# -----------------------------
[loader]
startup_order       = logger,rms,kwd,stt,ollama,llm,tts
health_timeout_s    = 30
health_interval_ms  = 200
# with ±20% jitter
restart_backoff_ms  = 250,500,1000,2000
# failures within 60s => FAIL
crash_loop_threshold= 3
# kill stale PID on port bind conflicts
port_hygiene        = true
vram_probe_cmd      = nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits
warmup_llm          = true
# call KWD /on-system-ready after all OK
greeting_on_ready   = true

[paths]
python              = python3

# Per-service launch definitions (used by loader)
[svc.logger]
name    = LOGGER
cmd     = python -m services.logger_service
port    = 5000

[svc.rms]
name    = RMS
cmd     = python -m services.rms_service
port    = 5006
health  = http://127.0.0.1:5006/health

[svc.kwd]
name    = KWD
cmd     = python -m services.kwd_service
port    = 5002
health  = http://127.0.0.1:5002/health

[svc.stt]
name    = STT
cmd     = python -m services.stt_service
port    = 5003
health  = http://127.0.0.1:5003/health

[svc.ollama]
name    = OLLAMA
cmd     = ollama serve
port    = 11434
# No /health endpoint that returns JSON like others
# loader will just check if process is alive
# (llm_service still does real availability check via /api/tags)

[svc.llm]
name    = LLM
cmd     = python -m services.llm_service
port    = 5004
health  = http://127.0.0.1:5004/health
warmup  = http://127.0.0.1:5004/warmup

[svc.tts]
name    = TTS
cmd     = python -m services.tts_service
port    = 5005
health  = http://127.0.0.1:5005/health

# -----------------------------
# RMS — Dynamic Background Noise
# -----------------------------
[rms]
bind_host           = 127.0.0.1
port                = 5006
sample_rate         = 16000
frame_ms            = 50
# rolling avg window (constraint)
window_sec          = 30
# 0..1 smoothing on dBFS
ema_alpha           = 0.2
# min | p05 | p10
noise_floor_mode    = p05
device              = default
shared_mode         = true
# push snapshots to logger
stats_interval_sec  = 5

[rms.deps]
logger_url          = http://127.0.0.1:5000

# -----------------------------
# KWD — Wake Word (openWakeWord)
# -----------------------------
[kwd]
bind_host           = 127.0.0.1
port                = 5002
model_path          = models/alexa_v0.1.onnx
# pre-adaptation
base_rms_threshold  = 0.050
cooldown_ms         = 1000
yes_phrases         = Yes?,Yes Master?,What's up?,I'm listening,Yo,Here!
# spoken once after all services ready
greeting_text       = Hi Master. Ready to serve.

[kwd.adaptive]
quiet_dbfs          = -60
noisy_dbfs          = -35
quiet_factor        = 0.80
noisy_factor        = 1.25

[kwd.deps]
logger_url          = http://127.0.0.1:5000
tts_url             = http://127.0.0.1:5005/speak
stt_url             = http://127.0.0.1:5003/start
rms_url             = http://127.0.0.1:5006/current-rms

# -----------------------------
# STT — Dialog Session Manager (faster-whisper)
# -----------------------------
[stt]
bind_host           = 127.0.0.1
port                = 5003
model_size          = small.en
# cuda | cpu
device              = cuda
# minimize VRAM
compute_type        = int8_float16
beam_size           = 1
vad_finalize_sec    = 2.0
chunk_length_s      = 15.0
follow_up_timer_sec = 4.0
word_timestamps     = false

[stt.adaptive]
quiet_dbfs          = -60
noisy_dbfs          = -35

[stt.vad]
silence_margin_db   = 8.0
energy_delta_db     = 6.0

[stt.deps]
logger_url          = http://127.0.0.1:5000
llm_url             = http://127.0.0.1:5004/complete
tts_ws_speak        = ws://127.0.0.1:5005/speak-stream
tts_ws_events       = ws://127.0.0.1:5005/playback-events
kwd_url             = http://127.0.0.1:5002/start
rms_url             = http://127.0.0.1:5006/current-rms

# -----------------------------
# LLM — Streaming Completions (Ollama)
# -----------------------------
[llm]
bind_host           = 127.0.0.1
port                = 5004
ollama_base_url     = http://127.0.0.1:11434
request_timeout_s   = 120
warmup_enabled      = true
warmup_text         = ok

# Default sampling (can be overridden per request)
temperature         = 0.6
top_p               = 0.9
top_k               = 40
repeat_penalty      = 1.05
max_tokens          = 512

[llm.deps]
logger_url          = http://127.0.0.1:5000

# -----------------------------
# TTS — Streaming Speech (Kokoro)
# -----------------------------
[tts]
bind_host           = 127.0.0.1
port                = 5005
# default voice (configurable)
voice               = af_heart
sample_rate         = 24000
device              = cuda
# minimize VRAM
dtype               = float16
max_queue_text      = 64
max_queue_audio     = 8
chunk_chars         = 120
silence_pad_ms      = 60
# primary mode is STT push
allow_llm_pull      = false

# ONNX assets
model_path          = models/kokoro-v1.0.fp16.onnx
voices_path         = models/voices-v1.0.bin
quant_preference    = fp16

[tts.deps]
logger_url          = http://127.0.0.1:5000
stt_ws_events_allow = true
