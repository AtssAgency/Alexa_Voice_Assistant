syntax = "proto3";

package aivoice.llm;

import "common.proto";

// Large Language Model service
service LlmService {
  rpc Configure(LlmConfig) returns (ConfigureResponse);
  rpc Complete(UserQuery) returns (stream LlmChunk);
}

message LlmConfig {
  string endpoint = 1;  // "localhost:11434"
  string model = 2;     // "llama3.1:8b-instruct-q4"
  string modelfile_path = 3;
  int32 request_timeout_ms = 4;
  int32 stream_chunk_chars = 5;
}

message ConfigureResponse {
  bool success = 1;
  string message = 2;
}

message UserQuery {
  string text = 1;
  string dialog_id = 2;
  int32 turn = 3;
}

message LlmChunk {
  string text = 1;
  bool eot = 2;  // end of turn
}